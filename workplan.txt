scan through the app and see what it's doing.

I want to change the pipeline to the following:
The handler_rl file contains the rl iterations and therapist workflow that wires the modules together, but does not contain asking or evaluating methods. 
These should be in the modules. The handler_rl file should only call the modules to do the work, passing the necessary parameters.
Each module should be a self-contained class that can be instantiated and called, and should designate its own model and api key (althogh we use the same model and api key for now).

The wired files realize the following workflow:
1. RL_handler intiate RL iterations and read Q table based on subject ID, if Q table is not found, it will intiate a new Q table based on config value
2. Questioner reads question lib
3. Questioner start an Dimension mask that marks if an Dimension was asked (starts with config value, which is all False for now).
4. if there is at least one Dimension have not been asked, Questioner picks an un-asked Dimension with highest Q value (epsilon greedy)
    4.1 Questioner's Rephraser runs an Openai LLM to rewrite the quesion structure (promoting variance in question wording)
    4.2 Response_analyzer GETs the User's Response and segments it into Segments (sentences)
    4.3 Response_analyzer analyzes each Segment, and for each Segment:
        4.3.1 If a Segment contains “yes”, “I don’t know”, “stop”, “maybe”, or “I don’t understand”, Response_analyzer assigns (Dimension, Score={0,1,2}) to the Segment by checking a preset table
            4.3.1.1 If "stop", break the loop and jump to 4.6
        4.3.2 If a Segment is general answer, Response_analyzer uses Openai LLM to find a {Dimension, Score} pair that best matches the Segment, with (NA,99) as un-related to any Dimension
    4.4 If all Segments are un-related to any Dimension, Response_analyzer uses Openai LLM to ask(POST) the User to re-try the Response, but only re-try once
        4.4.1 Do 4.2-4.3.2 again. If still all Segments are un-related to any Dimension, jump to 4.6
    4.5 Among all {Dimension, Score} pairs, for those Dimensions that were not asked yet and corresponding Scores are 2:
        4.5.1 Reflection-Validation's (RV) ReflectiveSummerizer uses an Openai LLM to provide a Simple_Summary of the Dimension, which rephrases the User's response segment from first person to third person
        4.5.2 RV POSTs the Simple_Summary to the User to GET the User's Follow_Up_Response
            4.5.2.1 If "stop", break the loop and jump to 4.6
        4.5.3 RV's Reasoner uses Openai LLM to determine if the User's Follow_Up_Response is related to the Dimension or Response (is valid)
            4.5.3.1 If valid, RV's Validator uses Openai LLM to POST an Empathic_Validation
            4.5.3.2 If not, RV's Guide uses Openai LLM to POST the User to re-try the Follow_Up_Response, but only re-try once
                4.5.3.2.1 Do 4.5.2.1-4.5.3.1 again. If still not valid, continue looping 4.5
    4.6 RL_handler updates Q table based on the collection of {Dimension, Score} pairs (Scores are the rewards for state Dimensions) for all Segments
    4.7 RL_handler marks the Dimensions in {Dimension, Score} pairs as asked, and marks the Dimension chosen in 4.1 as asked
5. CBT uses Openai LLM to read the full conversation history and POSTs the User a summary of the conversation, and asks the User to choose a Dimension to work on among those recieved Scores=2, if there are any.
    5.1 If there are no Dimensions to work on, jump to 8
6. CBT Gets the User's choice
    6.1 If User responds "stop", jump to 8
    6.2 If User's response contains number or dword matching the Dimension label, choose the Dimension
    6.3 If User's response contains a word not matching the Dimension label, use Openai LLM to ask the User to re-try the Response, but only re-try once
7. CBT's Questioner uses Openai LLM to generate questions for stage_1 objective
    7.1 Reasoner uses Openai LLM to evaluate the user’s response for its validity
        7.1.1 If valid, CaiTI progresses to the next stage
        7.1.2 If not, the Guide uses Openai LLM to aid the user in crafting a valid response by identifying issues in the current response and suggesting possible improvements for a valid answer.
            7.1.2.1 Only re-try twice. If still not valid, conclude the CBT process and CBT POSTs that the user seek professional assistance for a more effective and valid CBT experience.
8. RL_handler records the conversation history in a file bases on test subject ID and time, then conlcude the session.


   There should be dedicated util modules for atomic I/O (with wait time and multi-try for each I/O)
   There should be dedicated dedicated modules for LLM API operations
   There should be a continuously maintained last_user_response cache, dimension_score_collection cache, and full_conversation cache for the workflow